---
title: 'Week 3: Sampling Distributions and Hypothesis Testing'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jmv)
```

## Sampling Error

The degree to which the difference between the population mean and sample mean are "due to chance", which can be answered using hypothesis testing and inferential statistics.

## Sampling Distributions

-   Identify the degree of variability from one sample to another as a result of the sampling error.
-   From an infinite number of samples, with plotted means of these samples, a normal distribution of means would be the result.
-   The standard deviation of a sampling mean is known as **standard error of differences between means**, and shows the average distance between a sample mean and a population mean.
-   The standard deviation of a set of means is known as the standard error.

The formula for calculating standard error is:

$$
  \begin{aligned}
    \frac{s}{\sqrt{n}}
  \end{aligned}
$$

In summary, here are a few points that can help with your study into sampling distributions:

-   A random sample should represent the population well so sample statistics from a random sample should provide reasonable estimates of the population values.
-   There is always some error in estimating population values from sample statistics (sampling error).
-   If we take repeated samples from a population and calculate the mean, these mean scores will vary and form a distribution (distribution of sample means) which will itself have a mean and standard deviation.
-   The degree to which the sample means vary from the overall population mean (i.e. the standard deviation of the sampling distribution) is the standard error.
-   Larger samples provide more information than smaller samples so statistics from large samples have less standard error than statistics from smaller samples.

## Hypothesis Testing

The information provided by the sampling distribution of the mean can be used to test hypotheses. Hypotheses are specific predictions which can be subject to an empirical and statistical test. For example, suppose we wanted to know whether the mean intelligence score of 109 we have obtained from our sample of 50 people is different from the general population mean of 100. If our sample was special in some way (e.g. all high functioning people) we might hypothesise that there would be a difference. Using the sampling distribution of the mean, we can test whether such a sample could reasonably have arisen had we drawn our sample from a population in which μ = 100. That is, what is the probability of finding a sample which scores 109 on average, given a population mean of 100? We might find that the probability of obtaining a score of 109 is 0.20, which is a fairly high probability. Thus, a sample mean that high is often obtained from a population with a mean of 100—we would conclude there is no statistical difference between our sample mean and the population mean.

### Hypothesis Testing Steps

The steps that will be used throughout this unit when we are testing hypotheses using inferential statistics are as follows:

1.    Begin with a research hypothesis (i.e. that there is a difference in intelligence between our sample and the population mean; that is, it is unlikely that our sample comes from a population whose mean is 100).
2.    Set up the null hypothesis (i.e. that there is no difference between our sample and the population mean; that is, our sample mean comes from a population whose mean is 100). More to come on the null hypothesis in the next section!
3.    Construct the sampling distribution of the particular statistic (depending on the type of statistical technique you are going to use) on the assumption that the null hypothesis is true.
4.    Conduct the experiment and collect data.
5.    Compare the sample statistic you obtain to the sampling distribution.
6.    Evaluate the null hypothesis according to the probability of the statistic. That is, how probable is it that our sample mean comes from a distribution whose mean is 100? If it is highly unlikely, then we can reject the null hypothesis and conclude that there is a difference.

### The Null Hypothesis

The null hypothesis is the primary starting point for any statistical test. Basically, the null hypothesis states that there is no difference between the groups, or that there is no relationship between two or more variables. In more specific terms and using our intelligence testing example, it states that the data we have obtained in our sample (mean = 109) are likely to have come from the population with a mean intelligence score of 100. The subsequent statistical tests that we employ are used to test the likelihood of this.

Therefore, the probability/likelihood we are referring to is always the probability of obtaining that particular result given that the null hypothesis is true (see Howell, p. 90 ‘The first stumbling block’).

You should be familiar with the notation for the null hypothesis. In this case:

$$
  \begin{aligned}
	  H_0: μ = 100
	\end{aligned}
$$

This states the null hypothesis ($H_0$) that the sample we have drawn comes from a population with a mean of 100. If it is highly unlikely that the data we have obtained come from a distribution with a mean of 100 then we can simply reject the null hypothesis (that there is no difference) and conclude that there is a difference between our sample mean and that of the population.

What happens if our data do not lead us to reject the null hypothesis? That is, often the statistics tell us that our sample mean is no different from the population (e.g. the difference could be just due to chance). Thus, the null hypothesis is ‘proved’ or ‘supported’. In these cases it is best to say that we fail to reject the null hypothesis.
In summary, the null hypothesis states that there is no difference between groups, or no relationship between variables (i.e. null effect). The statistical test evaluates the likelihood of obtaining the data we have (e.g. mean of 109 on intelligence) if we took this sample from a population with a mean of 100.

### Hypothesis Testing Using the Normal Distribution

The following example is similar to the one provided by Howell. The principles are the same; however, illustrating the technique with a slightly different data set and outcome will be helpful in consolidating your understanding.

Research psychologists specialising in cognitive processes often study attention and the influence of attentional biases on a range of behaviours. One such behaviour is drug and alcohol use. Evidence shows that dependent drinkers allocate a large amount of their attention toward stimuli associated with drinking (e.g. pictures of alcohol, alcohol advertisements).

A computerised test of this has been developed which assesses the number of milliseconds a person takes to respond to the colour of words that are associated with alcohol represented on screen. Theoretically, the longer it takes them to name the colour of the word, the more attention is being drawn to the word itself. Researchers know that in the normal population, response times are normally distributed with a mean of 900 milliseconds and a standard deviation of 120 milliseconds.

Suppose an individual is referred to a cognitive psychologist with a score of 1100 milliseconds. Is this score sufficiently longer than the mean population response time to assume that this person did not come from a population of ‘normal’ people?

Here is the distribution of scores (sample means) on this task in the normal population ($\mu = 900, \sigma = 120$). The sample score we have obtained is also indicated under the distribution.

The first step here is to state the null hypothesis:

H0: There is no difference between our sample mean and the population mean. That is, the individual’s score does come from the population of ‘normal’ attention scores.

Next we can calculate the probability of scoring 1100 given a normal population mean of 900 (standard deviation of 120). That is, we can now calculate the probability that a score as high as this would be obtained from this population. All we need to do is calculate the z score for this observed value and determine the probability using the normal distribution table (or z tables, as we did in Topic 2).

$$
  \begin{aligned}
    z &= \frac{X - \mu}{\sigma}\\
    &= \frac{1100 - 900}{120}\\
    &= 1.67
  \end{aligned}
$$

Thus, we have a z score of 1.67. If we consult the z tables (Appendix z in Howell) we will see that the probability of a z score of 1.67 or above is 0.0475. The smaller portion column in the z table tells us the proportion of the population that score at that point and above. Therefore, less than 5% of the population score 1100 milliseconds or longer on this task. Statistically speaking, the probability that this score comes from a population with a mean of 900 is less than 5%. Is this likelihood sufficiently small? How do we decide if we can reject the null hypothesis based on this information?

### Decision Making

 To answer this question, we need to compare our obtained probability value to some standard. The conventional approach to this is to use the 0.05 level as a ‘cut off’. This is referred to as the rejection or significance level of the statistical test. If our probability level is less than or equal to this ‘rejection level’ then we reject H0. As noted by Howell, this is an arbitrary convention that has been established over the years for specific reasons but one that is pervasive throughout psychological research. Other significance levels are used (e.g. 0.01) for a variety of reasons. At this stage it is only necessary for you to know that the standard level of significance/rejection is 0.05.
 
The probability level we have obtained in the example is p = 0.0475. This is less than 0.05 and therefore, following the decision rule, we can reject the null hypothesis. Thus, we can say that it is highly unlikely that the person came from the ‘normal population’ and conclude that the score of 1100 is sufficiently different from the normal population mean.

In this case, we have supported the alternative hypothesis, which states that there is a difference between the sample mean and the population mean. This is in essence, the opposite to the null hypothesis:

As you know, the null hypothesis is that of no difference:

$$
  \begin{aligned}
    H_0: \mu = 900
  \end{aligned}
$$

The alternative hypothesis is that there is a difference:

$$
  \begin{aligned}
    H_1: \mu \ne 900
  \end{aligned}
$$

Or, that the mean is higher than or lower than the population mean:

$$
  \begin{aligned}
    H_1: \mu \gt 900\\
    H_1: \mu \lt 900
  \end{aligned}
$$

## Type I and II errors

There is always some error associated with decision making in everyday life. However, as noted by Howell, in statistics we can precisely identify the probability of making an error in our decision making processes.

For example, above we rejected the null hypothesis and concluded that the person did not come from the ‘normal’ population. That is, the probability of obtaining a score of 1100 milliseconds was sufficiently small (i.e. less than 0.05) as to be unlikely to have come from the ‘normal’ population.

However the fact that we are using the 0.05 ‘rejection’ level as our cut off produces a degree of error: a 5% degree of error to be exact.

A **Type I** error is when we erroneously reject a null hypothesis. It is the probability of rejecting a ‘true’ null hypothesis (e.g. the probability of saying there is a difference when there in fact isn’t). The risk of a Type I error is small and under the control of the researcher (it can be set at 0.01, for example).

A **Type II** errors occurs when we fail to reject a false null hypothesis. So whenever we fail to reject a null hypothesis there is a risk of a Type II error. This relates to the power of the test the probability of rejecting a null hypothesis when it is false.

The difference between Type I and Type II errors are illustrated in the table below for quick reference (see also Howell, p. 98).


|       Decision     |       $H_0$: True      |       $H_0$: False     |
|:------------------:|:-------------------:|:-------------------:|
|   Reject $H_0$       |   Type I error      |   Correct decision  |
|   Don’t reject $H_0$  |   Correct decision  |   Type II error     |

## One and Two Tailed Tests

One-tailed test: $\lt$ or $\gt$ (directional). For a one-tailed test, we use the full 5% rejection region in the ‘one-tail’, in this example the high tail. Thus, any scores that fall in the top 5% of scores are considered sufficiently unlikely.

> One-tailed test: $H_1: \mu \gt 900$

Two-tailed test: $\ne$ (measuring/comparing difference). For a two-tailed test we must separate the proportion into the two tails: 0.025 at one end, 0.025 at the other (so that the overall probability level is set at 0.05).

> Two-tailed test: $H_1: \mu \ne 900$





